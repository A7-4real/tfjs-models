{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BACH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_faZN4olNIv_",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-tcZ-WKNO2p",
        "colab_type": "code",
        "outputId": "43a84e6e-ddbc-4990-9a34-195e3111208a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ./gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia4sVe28NVVz",
        "colab_type": "text"
      },
      "source": [
        "The patches extracted from the images are stored in model1bmi/train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjNmmulnNi9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = './train/'\n",
        "test_path = './test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHUuJLz2Nq7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrXe6ZSGVDq9",
        "colab_type": "text"
      },
      "source": [
        "# Getting dataset ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqhe0FpRVImN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://rdm.inesctec.pt/dataset/604dfdfa-1d37-41c6-8db1-e82683b8335a/resource/df04ea95-36a7-49a8-9b70-605798460c35/download/breasthistology.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDROTdnIV7au",
        "colab_type": "text"
      },
      "source": [
        "The dataset is from https://rdm.inesctec.pt/dataset/nis-2017-003. The above zip is password-protected. Request the password from therein and extract it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTW3O6QSW7o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir train/Normal train/Benign train/Invasive train/InSitu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5rHnf_gXDvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Credits: ICIAR2018 by ImagingLabs\n",
        "class PatchExtractor:\n",
        "    def __init__(self, img, patch_size, stride):\n",
        "        '''\n",
        "        :param img: :py:class:`~PIL.Image.Image`\n",
        "        :param patch_size: integer, size of the patch\n",
        "        :param stride: integer, size of the stride\n",
        "        '''\n",
        "        self.img = img\n",
        "        self.size = patch_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def extract_patches(self):\n",
        "        \"\"\"\n",
        "        extracts all patches from an image\n",
        "        :returns: A list of :py:class:`~PIL.Image.Image` objects.\n",
        "        \"\"\"\n",
        "        wp, hp = self.shape()\n",
        "        return [self.extract_patch((w, h)) for h in range(hp) for w in range(wp)]\n",
        "\n",
        "    def extract_patch(self, patch):\n",
        "        \"\"\"\n",
        "        extracts a patch from an input image\n",
        "        :param patch: a tuple\n",
        "        :rtype: :py:class:`~PIL.Image.Image`\n",
        "        :returns: An :py:class:`~PIL.Image.Image` object.\n",
        "        \"\"\"\n",
        "        return self.img.crop((\n",
        "            patch[0] * self.stride,  # left\n",
        "            patch[1] * self.stride,  # up\n",
        "            patch[0] * self.stride + self.size,  # right\n",
        "            patch[1] * self.stride + self.size  # down\n",
        "        ))\n",
        "\n",
        "    def shape(self):\n",
        "        wp = int((self.img.width - self.size) / self.stride + 1)\n",
        "        hp = int((self.img.height - self.size) / self.stride + 1)\n",
        "        return wp, hp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgYb9_KTXNLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "LABELS = ['Normal', 'Benign', 'InSitu', 'Invasive']\n",
        "IMAGE_SIZE = (2048, 1536)\n",
        "PATCH_SIZE = 512\n",
        "\n",
        "# This is the folder you'll find after extracting the dataset.\n",
        "train_folder = './Training_data'\n",
        "labels = {name: LABELS[index] for index in range(len(LABELS)) for name in glob.glob(train_folder + '/' + LABELS[index] + '/*.tif')}\n",
        "\n",
        "for key, value in labels.items():\n",
        "  try:\n",
        "    with Image.open(key) as img:\n",
        "      # the patch-size and stride is according to the paper\n",
        "      extractor = PatchExtractor(img=img, patch_size=PATCH_SIZE, stride=256)\n",
        "      patches = extractor.extract_patches()\n",
        "      count = 0\n",
        "      for p in patches:\n",
        "        count += 1\n",
        "        # print('./train/' + value + '/' + str(count) + '_' + key.split('/')[-1])\n",
        "        p.save('./train/' + value + '/' + str(count) + '_' + key.split('/')[-1])\n",
        "  except Exception as error:\n",
        "    print('error with', key, error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__fUu7MjYcuT",
        "colab_type": "text"
      },
      "source": [
        "This will prepare a dataset with patches of size 512x512. 35 from each image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmJIBkDOOowG",
        "colab_type": "code",
        "outputId": "0e5ec16b-fdd8-4379-d435-5575c668ba49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "aug = ImageDataGenerator(rotation_range=360, brightness_range=(0.5, 1.5), horizontal_flip=True, vertical_flip=True, validation_split=0.15)\n",
        "\n",
        "train_generator = aug.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(512, 512),\n",
        "    batch_size=bs,\n",
        "    subset='training') # set as training data\n",
        "\n",
        "validation_generator = aug.flow_from_directory(\n",
        "    train_path, # same directory as training data\n",
        "    target_size=(512, 512),\n",
        "    batch_size=bs,\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# test_generator = test_datagen.flow_from_directory(\n",
        "#         test_path,\n",
        "#         target_size=(512, 512),\n",
        "#         batch_size=bs,\n",
        "#         class_mode='categorical', shuffle=False)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7067 images belonging to 4 classes.\n",
            "Found 1244 images belonging to 4 classes.\n",
            "Found 1260 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6d79fd8b-5aec-414d-ce09-452cd2a5c852",
        "id": "YqK_Rt6s4g0t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Benign': 0, 'InSitu': 1, 'Invasive': 2, 'Normal': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNruU1rGNu9V",
        "colab_type": "text"
      },
      "source": [
        "# Defining & Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao3OqvuzN8Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa5nrVV0N9mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re1RS-V8OKqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PatchWiseNet:\n",
        "  @staticmethod\n",
        "  def build(width, height, depth, classes, channels=1):\n",
        "    # initialize the model along with the input shape to be\n",
        "    # \"channels last\" and the channels dimension itself\n",
        "    model = Sequential()\n",
        "    pad = 'same'\n",
        "    inputShape = (height, width, depth)\n",
        "    chanDim = -1\n",
        "\n",
        "    # if we are using \"channels first\", update the input shape\n",
        "    # and channels dimension\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "      inputShape = (depth, height, width)\n",
        "      chanDim = 1\n",
        "\n",
        "    #       block 1\n",
        "    model.add(Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding=pad, input_shape=inputShape))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=16, kernel_size=(2, 2), strides=2))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #     block 2\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(2, 2), strides=2))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #     block 3\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(2, 2), strides=2))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #     block 4\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #     block 5\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=pad))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters=channels, kernel_size=(1, 1), strides=1))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxZCmfSCOXj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = PatchWiseNet.build(width=512, height=512, depth=3, classes=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5VGIKcYOhT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLxGXPV3O4a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "import math\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "sgd = optimizers.SGD(lr=0.001)\n",
        "\n",
        "filepath=\"./models/weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "\n",
        "mcp = callbacks.ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [mcp]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFeOYfFyQMnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R--2rzjtQYWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step=train_generator.n//train_generator.batch_size\n",
        "valid_step=validation_generator.n//validation_generator.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R19mBdPwQR0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_step,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=valid_step,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxhGk6l6onie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment to load previously trained model\n",
        "# from keras.models import load_model\n",
        "# model = load_model('./models/weights-04-0.35.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00hv41dgsxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThLDF_PGjNmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c105b90d-ee1c-4599-97a2-49b9c715101c"
      },
      "source": [
        "I = numpy.asarray(PIL.Image.open('./test/Invasive/10_15.tif'))\n",
        "model.predict(I.reshape(-1,512, 512, 3) , verbose=1).argmax(axis=1)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 0s 26ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jssRg4NwjYJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "436f26c4-5f89-49d2-a906-1a4ebf461c76"
      },
      "source": [
        "train_generator.class_indices"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Benign': 0, 'InSitu': 1, 'Invasive': 2, 'Normal': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTIu3wKnapOC",
        "colab_type": "text"
      },
      "source": [
        "# Convert the  Keras model to TF.js Layers format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1AqYUguQ6Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflowjs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUXl9FledLEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(model, tfjs_target_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtL1IbtgdQ-W",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "--- OR ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V54reVMB_RI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorflowjs_converter --input_format keras \\\n",
        "                       ./models/weights-10-0.79.hdf5 \\\n",
        "                       path/to/tfjs_target_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx_mCTCjZlDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If the above runs into some import errors, use it with tensorflow v0.6.4 and keras v2.1.6"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}